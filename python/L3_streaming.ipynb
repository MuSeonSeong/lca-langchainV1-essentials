{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "483ab18e-f419-46ad-9bbe-171ffd05f983",
   "metadata": {},
   "source": [
    "# Streaming\n",
    "\n",
    "Streaming...\n",
    "\n",
    "There are two types frequently used with Agents:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "166fc0b1-2322-4dd2-a358-89309fb9f4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.chat_models import init_chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a26b4586-453a-4c10-9fae-4be3f4cb6cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_chat_model(\"gpt-5-mini\", model_provider=\"openai\")\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[],\n",
    "    system_prompt=\"You are a full-stack comedian\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a907aa9-a608-47e2-92d4-6758a1728cb2",
   "metadata": {},
   "source": [
    "## No Steaming (invoke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc384cf0-b208-4ab1-b7e2-f4b93dab08bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do programmers prefer dark mode?\n",
      "\n",
      "Because light attracts bugs.\n",
      "\n",
      "Want another one?\n"
     ]
    }
   ],
   "source": [
    "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Tell me a joke\"}]})\n",
    "print(result[\"messages\"][1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4d7975-8d94-4d5e-8493-e68ac9fcedf9",
   "metadata": {},
   "source": [
    "## values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e30a2273-1dd3-47e8-a38e-05ed4b750000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Tell me a joke\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Why did the full-stack developer go broke?\n",
      "\n",
      "Because he used up all his cache. \n",
      "\n",
      "Want another one?\n"
     ]
    }
   ],
   "source": [
    "# Stream = values\n",
    "for step in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Tell me a joke\"}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada88835-3c66-4241-b3d9-4f3d38390c86",
   "metadata": {},
   "source": [
    "## message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1c07c50-7b96-4446-9065-0733e1b0b308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node: model\n",
      "content: \n",
      "node: model\n",
      "content: Why\n",
      "node: model\n",
      "content:  do\n",
      "node: model\n",
      "content:  programmers\n",
      "node: model\n",
      "content:  prefer\n",
      "node: model\n",
      "content:  dark\n",
      "node: model\n",
      "content:  mode\n",
      "node: model\n",
      "content: ?\n",
      "node: model\n",
      "content:  Because\n",
      "node: model\n",
      "content:  light\n",
      "node: model\n",
      "content:  attracts\n",
      "node: model\n",
      "content:  bugs\n",
      "node: model\n",
      "content: .\n",
      "node: model\n",
      "content: \n"
     ]
    }
   ],
   "source": [
    "for token, metadata in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Tell me a joke.\"}]},\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    print(f\"node: {metadata['langgraph_node']}\")\n",
    "    print(f\"content: {token.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a9cc553-7357-4d36-b88d-25eaf7462cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I write in caffeine, commit in sleep,\n",
      "push mornings to a branch where bugs creep.\n",
      "My heart runs on semicolons and late-night tabs,\n",
      "console.log confessions, tiny unwrapped jabs.\n",
      "\n",
      "The server hums like a distant choir,\n",
      "404s like lost socks, retries on wire.\n",
      "I stitch together pages with duct-tape code,\n",
      "and pray the stylesheet forgives what I broke.\n",
      "\n",
      "You arrive as a viewport, resized and new,\n",
      "responsive in smile, graceful in hue.\n",
      "We cache quiet moments, expire old pain,\n",
      "refresh for laughter, GET love again.\n",
      "\n",
      "If life throws exceptions, I’ll catch with a grin—\n",
      "deploy a new version: we begin again."
     ]
    }
   ],
   "source": [
    "for token, metadata in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Write me a poem.\"}]},\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    print(f\"{token.content}\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c4477-24ff-4321-8f50-aff3324fa831",
   "metadata": {},
   "source": [
    "## Tools can stream too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c68179e6-d388-494a-b10d-109c230f6ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('updates', {'model': {'thread_model_call_count': 1, 'run_model_call_count': 1, 'messages': [AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 132, 'total_tokens': 220, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 64, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CNnmt0XAk1DNhRy2vJ7pyM4dONEfq', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--f680c744-5ef7-4887-917c-ca2a20609cab-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_cbNoBFsPtTGQUAEdEaadFfP6', 'type': 'tool_call'}], usage_metadata={'input_tokens': 132, 'output_tokens': 88, 'total_tokens': 220, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 64}})]}})\n",
      "('custom', 'Looking up data for city: San Francisco')\n",
      "('custom', 'Acquired data for city: San Francisco')\n",
      "('updates', {'tools': {'messages': [ToolMessage(content=\"It's always sunny in San Francisco!\", name='get_weather', id='fdfd733a-fcc5-493f-a9a9-c0229521d661', tool_call_id='call_cbNoBFsPtTGQUAEdEaadFfP6')]}})\n",
      "('updates', {'model': {'thread_model_call_count': 2, 'run_model_call_count': 2, 'messages': [AIMessage(content=\"It's always sunny in San Francisco!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 168, 'total_tokens': 178, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CNnmv7qpTZTsaRMwCDfQRsOLePiSz', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--d4fa9d55-e95d-4567-a86f-4db4f95dc2e8-0', usage_metadata={'input_tokens': 168, 'output_tokens': 10, 'total_tokens': 178, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}})\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langgraph.config import get_stream_writer\n",
    "\n",
    "\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    writer = get_stream_writer()\n",
    "    # stream any arbitrary data\n",
    "    writer(f\"Looking up data for city: {city}\")\n",
    "    writer(f\"Acquired data for city: {city}\")\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-5-mini\",\n",
    "    tools=[get_weather],\n",
    ")\n",
    "\n",
    "for chunk in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n",
    "    stream_mode=[\"updates\", \"custom\"],\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4d7ef47-e857-4e07-a233-888306e3e0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking up data for city: San Francisco\n",
      "Acquired data for city: San Francisco\n"
     ]
    }
   ],
   "source": [
    "for chunk in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n",
    "    stream_mode=[\"updates\", \"custom\"],\n",
    "):\n",
    "    if chunk[0] == \"custom\":\n",
    "        print(chunk[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072fb561-fc30-4095-bfc5-8a35809aa307",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
